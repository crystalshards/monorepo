---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: crystalshards-alerts
  namespace: monitoring
  labels:
    app: crystalshards
    release: prometheus-operator
spec:
  groups:
  - name: crystalshards.rules
    rules:
    
    # High error rate alert
    - alert: HighErrorRate
      expr: sum(rate(http_requests_total{status=~"5..", job=~"crystalshards-registry|crystaldocs|crystalgigs"}[5m])) / sum(rate(http_requests_total{job=~"crystalshards-registry|crystaldocs|crystalgigs"}[5m])) > 0.05
      for: 2m
      labels:
        severity: warning
        service: crystalshards
      annotations:
        summary: "High error rate detected"
        description: "Error rate is {{ $value | humanizePercentage }} for the last 5 minutes"

    # High response time alert
    - alert: HighResponseTime
      expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{job=~"crystalshards-registry|crystaldocs|crystalgigs"}[5m])) by (le)) > 2
      for: 5m
      labels:
        severity: warning
        service: crystalshards
      annotations:
        summary: "High response time detected"
        description: "95th percentile response time is {{ $value }}s for the last 5 minutes"

    # Service down alert
    - alert: ServiceDown
      expr: up{job=~"crystalshards-registry|crystaldocs|crystalgigs"} == 0
      for: 1m
      labels:
        severity: critical
        service: crystalshards
      annotations:
        summary: "Service {{ $labels.job }} is down"
        description: "{{ $labels.job }} has been down for more than 1 minute"

    # Database connection issues
    - alert: DatabaseConnectionsHigh
      expr: pg_stat_activity_count{job="postgresql-cluster"} > 80
      for: 5m
      labels:
        severity: warning
        service: postgresql
      annotations:
        summary: "High database connections"
        description: "PostgreSQL has {{ $value }} active connections"

    # Redis memory usage
    - alert: RedisMemoryHigh
      expr: redis_memory_used_bytes{job="redis-cluster"} / redis_memory_max_bytes{job="redis-cluster"} > 0.8
      for: 5m
      labels:
        severity: warning
        service: redis
      annotations:
        summary: "High Redis memory usage"
        description: "Redis memory usage is {{ $value | humanizePercentage }}"

    # Pod CPU usage
    - alert: PodCPUUsageHigh
      expr: sum(rate(container_cpu_usage_seconds_total{namespace=~"crystalshards|crystaldocs|crystalgigs"}[5m])) by (pod) > 0.8
      for: 10m
      labels:
        severity: warning
        service: kubernetes
      annotations:
        summary: "High CPU usage for pod {{ $labels.pod }}"
        description: "Pod {{ $labels.pod }} CPU usage is {{ $value | humanizePercentage }}"

    # Pod memory usage
    - alert: PodMemoryUsageHigh
      expr: container_memory_usage_bytes{namespace=~"crystalshards|crystaldocs|crystalgigs"} / container_spec_memory_limit_bytes > 0.8
      for: 5m
      labels:
        severity: warning
        service: kubernetes
      annotations:
        summary: "High memory usage for pod {{ $labels.pod }}"
        description: "Pod {{ $labels.pod }} memory usage is {{ $value | humanizePercentage }}"

    # Documentation build failures
    - alert: DocBuildFailures
      expr: increase(doc_build_failures_total{job="crystaldocs"}[10m]) > 3
      for: 1m
      labels:
        severity: warning
        service: crystaldocs
      annotations:
        summary: "Multiple documentation build failures"
        description: "{{ $value }} documentation builds have failed in the last 10 minutes"

    # Payment processing failures
    - alert: PaymentProcessingFailures
      expr: increase(payment_failures_total{job="crystalgigs"}[10m]) > 2
      for: 1m
      labels:
        severity: critical
        service: crystalgigs
      annotations:
        summary: "Payment processing failures detected"
        description: "{{ $value }} payment processing failures in the last 10 minutes"

    # Search performance degradation
    - alert: SearchPerformanceDegraded
      expr: histogram_quantile(0.95, sum(rate(search_duration_seconds_bucket{job="crystalshards-registry"}[5m])) by (le)) > 5
      for: 3m
      labels:
        severity: warning
        service: crystalshards-registry
      annotations:
        summary: "Search performance degraded"
        description: "95th percentile search time is {{ $value }}s"

    # MinIO storage usage
    - alert: MinIOStorageHigh
      expr: minio_bucket_usage_total_bytes{job="minio-tenant"} / (1024^3) > 5  # 5GB threshold
      for: 5m
      labels:
        severity: warning
        service: minio
      annotations:
        summary: "High MinIO storage usage"
        description: "Bucket {{ $labels.bucket }} is using {{ $value | humanize }}B of storage"

---
# Create a PodMonitor for our applications to expose custom metrics
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: crystalshards-pods
  namespace: monitoring
  labels:
    app: crystalshards
    release: prometheus-operator
spec:
  namespaceSelector:
    matchNames:
    - crystalshards
    - crystaldocs
    - crystalgigs
  selector:
    matchLabels:
      metrics: "enabled"
  podMetricsEndpoints:
  - port: http
    path: /metrics
    interval: 30s
    scrapeTimeout: 10s